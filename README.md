ğŸ§  DCGAN â€” Deep Convolutional GAN for Multi-Dataset Image Generation
A Stability-Oriented GAN Implementation on CelebA, MNIST & SVHN

This repository presents a custom Deep Convolutional Generative Adversarial Network (DCGAN) designed to evaluate adversarial training stability across datasets of varying complexity â€” from handwritten grayscale digits to real-world face images.

ğŸš€ Project Highlights

âœ” Stability-focused GAN training
âœ” Supports CelebA, MNIST, and SVHN datasets
âœ” Binary cross-entropy loss for improved convergence
âœ” Tools for diagnosing mode collapse and adversarial imbalance
âœ” Suitable for machine learning research, demonstrations, and academic papers

ğŸ“Œ Key Research Goals

This project studies how DCGAN stability is affected by:

Dataset complexity differences

Generator vs. discriminator learning balance

Batch sizes and label smoothing

Latent-space structure & expressiveness

Training logs include:

Loss evolution (G-loss vs D-loss)

Discriminator accuracy trends

Gradient norm monitoring

Latent-space interpolation

Discriminator score distributions

ğŸ—‚ï¸ Supported Datasets
Dataset	Image Size	Characteristics	Storage Path
CelebA	64Ã—64 RGB	Human faces	datasets/celeba/img_align_celeba/
MNIST	28Ã—28 grayscale	Handwritten digits	Dataset/mnist/
SVHN	32Ã—32 RGB	Street-view digits	Dataset/SVHN/

Note: CelebA must be downloaded manually due to license restrictions.

ğŸ› ï¸ Environment & Dependencies

Tested using:

Python â‰¥ 3.8

TensorFlow / Keras

NumPy

Matplotlib

tqdm

scikit-image

Install all dependencies:

```bash
pip install tensorflow numpy matplotlib tqdm scikit-image
```

ğŸ§± Model Architecture
ğŸ”¹ Generator

Dense projection + reshape

Series of Conv2DTranspose (upsampling) blocks

BatchNorm + LeakyReLU

Tanh output (normalized image limits)

ğŸ”¸ Discriminator

Conv2D downsampling layers

LeakyReLU activations

Optional dropout

Sigmoid final prediction

ğŸ’¡ Binary cross-entropy â†’ more stable than MSE used in LSGAN

ğŸ‹ï¸ Training Strategy
1ï¸âƒ£ Discriminator Update

Real images â†’ label 1
Fake images â†’ label 0
Supports label smoothing for stability

2ï¸âƒ£ Generator Update

Goal â†’ fool the discriminator into predicting real labels

ğŸ“Š Per-epoch logging:

D-Loss & G-Loss

Accuracy for real/fake samples

Generated image sampling

All plots and evaluation results are automatically saved.

ğŸ“ˆ Outputs & Analysis Tools

Generated diagnostics include:

Tool	Purpose
Loss curves	Indicator of convergence stability
Discriminator accuracy curve	Detecting imbalance (ideal: 45â€“55%)
Latent-space interpolation	Continuity of learned features
D-score histogram	Detects overconfidence
Gradient-norm plots	Spotting collapse or exploding gradients
Evaluation tables	Final quantitative comparisons
ğŸ“š Comparison With LSGAN (Research Paper)
Dataset	LSGAN Behavior	This Implementation
MNIST	Stable	Smoother convergence
SVHN	Often unstable	Stable using BCE
CelebA	Mode collapse common	Avoids collapse + balanced accuracy

ğŸ† Improvements due to:

Binary cross-entropy

BatchNorm + LeakyReLU

Label smoothing

Gradient norm supervision

â–¶ï¸ Run Training

CelebA:
```bash
python dcgan_stability_analysis.py --dataset celebA
```

MNIST:
```bash
python dcgan_stability_analysis.py --dataset mnist
```

SVHN:
```bash
python dcgan_stability_analysis.py --dataset svhn
```

Additional hyper-parameters (batch size, epochs, latent dimension) can be modified via command-line flags.

â— Troubleshooting
Issue	Cause	Fix
Black generated images	Generator collapse	Reduce LR / increase batch
D-accuracy â‰ˆ 1.0	Discriminator overpowering	Apply label smoothing / lower D-LR
Slow CelebA training	Large dataset	Reduce resolution or dataset size
Empty plots	Logging disabled	Move logging inside epoch loop
ğŸ™ Acknowledgements

Datasets:

CelebA â€” Chinese University of Hong Kong

SVHN â€” Stanford UFLDL Lab

MNIST â€” Yann LeCun et al.

References:

DCGAN (Radford et al.)

LSGAN research paper (baseline comparison)
